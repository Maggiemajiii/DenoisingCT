{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b32722",
   "metadata": {},
   "source": [
    "# EC500 - Group 6 - Denoising CT Images \n",
    "**Avantika Kothandaraman, Caiwei Zhang, Long Chen**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43fbfbd",
   "metadata": {},
   "source": [
    "## Section-1: Installing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pynrrd\n",
    "# !pip install SimpleITK\n",
    "# !python -c \"import monai\" || pip install -q \"monai\"\n",
    "# !python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221cb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install monai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install patchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c961adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import v2\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import nibabel as nib\n",
    "import nrrd\n",
    "from torchvision.datasets import ImageFolder\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from scipy import ndimage\n",
    "import tempfile\n",
    "import shutil\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971fd3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureChannelFirst,\n",
    "    Compose,\n",
    "    AsChannelLastd,\n",
    "    CropForegroundd,\n",
    "    ScaleIntensityd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    Invertd,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandSpatialCropSamples,\n",
    "    RandSpatialCropd,\n",
    "    ScaleIntensityRanged,\n",
    "    ScaleIntensityRange,\n",
    "    RandRotated,\n",
    "    RandFlipd,\n",
    "    RandZoomd,\n",
    "    RandScaleIntensityd, \n",
    "    RandShiftIntensityd,\n",
    "    #AddChannel,\n",
    "    ToTensord,\n",
    "    NormalizeIntensityd\n",
    ")\n",
    "from monai.handlers.utils import from_engine \n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference \n",
    "from monai.inferers import SlidingWindowInferer\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch, pad_list_data_collate\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "from patchify import patchify\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d220662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653f5f9a",
   "metadata": {},
   "source": [
    "## Section-2: Initial data inspection and experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef100fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing directory\n",
    "data_dir = \"./scans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526be702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping through for inspection\n",
    "count = 0\n",
    "dims = []\n",
    "sizes = []\n",
    "shapes = []\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.nrrd'):\n",
    "        count += 1\n",
    "        img, header = nrrd.read(os.path.join(data_dir,filename))\n",
    "        dims.append(img.ndim)\n",
    "        sizes.append(img.size)\n",
    "        shapes.append(img.shape)\n",
    "        \n",
    "dims_check = all(dim == dims[0] for dim in dims)\n",
    "size_check = all(size == sizes[0] for size in sizes)\n",
    "shape_check = all(shape == shapes[0] for shape in shapes)\n",
    "\n",
    "if dims_check and size_check and shape_check:\n",
    "    print('Dimensions, shapes and sizes are uniform')\n",
    "else:\n",
    "    print('Dimensions, shapes and sizes are NOT uniform')\n",
    "    \n",
    "print('The total number of images in the dataset is {}'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5466bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_2d(img_volume, axis=1):\n",
    "    return np.max(img_volume, axis=axis)\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((512,512)),\n",
    "    #transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume, header = nrrd.read(os.path.join(data_dir,'L506_signal.nrrd'))\n",
    "volume_2d = convert_to_2d(volume)\n",
    "volume_2d = trans(volume_2d)\n",
    "print(volume.shape, volume_2d.squeeze().shape, volume_2d.type)\n",
    "plt.imshow(volume_2d.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_2d = volume_2d.numpy()\n",
    "volume_2d.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d42a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patches(image):\n",
    "    demo_dict = []\n",
    "    image = image.squeeze()\n",
    "    patches = patchify(image.numpy(), (64,64), step=64)\n",
    "    #patches = patchify(image, (64,64), step=64)   \n",
    "    for i in range(patches.shape[0]):\n",
    "        for j in range(patches.shape[1]):\n",
    "            single_patch_img = patches[i,j,:,:]\n",
    "            demo_dict.append(single_patch_img)\n",
    "    return demo_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ac048",
   "metadata": {},
   "source": [
    "## Set deterministic seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583b37c3",
   "metadata": {},
   "source": [
    "## Section-3: Creating a custom dataset and making transforms for augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778446e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.data = []\n",
    "        image_sizes = []\n",
    "        \n",
    "        for file in os.listdir(self.root_dir):\n",
    "            if file.endswith('signal.nrrd'):\n",
    "                image_id = file.split('_')[0]\n",
    "                \n",
    "                # reading in the images\n",
    "                signal_nrrd, _ = nrrd.read(os.path.join(self.root_dir, file))\n",
    "                noise_nrrd, _ = nrrd.read(os.path.join(self.root_dir, f\"{image_id}_noise.nrrd\"))\n",
    "                std_nrrd, _ = nrrd.read(os.path.join(self.root_dir, f\"{image_id}_std.nrrd\"))\n",
    "\n",
    "                # converting to 2D Axial\n",
    "                signal = np.max(signal_nrrd, axis=1)\n",
    "                noise = np.max(noise_nrrd, axis=1)\n",
    "                std = np.max(std_nrrd, axis=1)\n",
    "                \n",
    "                # converting to tensor and resizing to 512,512 for uniformity\n",
    "                trans = transforms.Compose([transforms.ToTensor(), \n",
    "                                           transforms.Resize((512,512))])\n",
    "                                           #transforms.Normalize(mean=[0.5], std=[0.5]])\n",
    "                signal = trans(signal)\n",
    "                noise = trans(noise)\n",
    "                std = trans(std)\n",
    "                \n",
    "                # generating input image fromm signal and noise\n",
    "                k = random.uniform(0,5)\n",
    "                ct_generated = signal + (k*noise)\n",
    "                \n",
    "                # generating patches\n",
    "                ct_patches = patches(ct_generated)\n",
    "                std_patches = patches(std)\n",
    "                #print(len(ct_patches), len(std_patches))\n",
    "                \n",
    "                # storing the new dataset in a dictionary\n",
    "                for i in range(len(ct_patches)):\n",
    "                    self.data.append({'ct_generated' : ct_patches[i], 'std_map' : std_patches[i]})\n",
    "                \n",
    "                \n",
    "    def data_info(self, idx):\n",
    "        item = self.data[idx]\n",
    "        ct_gen = item['ct_generated']\n",
    "        std_ma = item['std_map']\n",
    "\n",
    "        # Print the index of the data item\n",
    "        print(f\"Data item {idx}:\")\n",
    "\n",
    "        # Print the shape of the ct_generated tensor\n",
    "        print(f\"ct_generated shape: {ct_gen.shape}\")\n",
    "\n",
    "        # Print the shape of the std_map tensor\n",
    "        print(f\"std_map shape: {std_ma.shape}\")\n",
    "\n",
    "        print()\n",
    "            \n",
    "            \n",
    "    def plot_ct(self, idx):\n",
    "        item = self.data[idx]\n",
    "        ct_generated = item['ct_generated']\n",
    "        std_map = item['std_map']\n",
    "        \n",
    "        # Create a figure\n",
    "        plt.figure(figsize=(5,5))\n",
    "        \n",
    "        # Plot axial view of ct_generated\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(ct_generated, cmap='gray')\n",
    "        plt.title('ct_generated Patch-0')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Plot axial view of std_map\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(std_map, cmap='hot')\n",
    "        plt.title('std_map Patch-0')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        print(\"Type of item:\", type(item))\n",
    "        print(\"Available keys:\", item.keys())\n",
    "        print(\"Type of image data:\", type(item['ct_generated']))  \n",
    "        print(\"Shape of image data:\", item['ct_generated'].shape)\n",
    "        ct_generated = torch.from_numpy(item['ct_generated']).unsqueeze(0).float()  \n",
    "        std_map = torch.from_numpy(item['std_map']).unsqueeze(0).float() \n",
    "        #return item\n",
    "        print(\"Shape of ct_generated:\", ct_generated.shape)\n",
    "        print(\"Shape of std_map:\", std_map.shape)\n",
    "        return {'ct_generated': ct_generated, 'std_map': std_map}\n",
    "    \n",
    "    def patches(image):\n",
    "        demo_dict = []\n",
    "        image = image.squeeze()\n",
    "        patches = patchify(image.numpy(), (64,64), step=64)\n",
    "\n",
    "        for i in range(patches.shape[0]):\n",
    "            for j in range(patches.shape[1]):\n",
    "                single_patch_img = patches[i,j,:,:]\n",
    "                demo_dict.append(single_patch_img)\n",
    "        return demo_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a9c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomData(root_dir = data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd12e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(custom_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b64617",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset.data_info(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbdc330",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset.plot_ct(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, remaining_files = train_test_split(custom_dataset, test_size=0.2, random_state=42)\n",
    "val_files, test_files = train_test_split(remaining_files, test_size=0.5, random_state=42)\n",
    "\n",
    "print(len(train_files), len(val_files), len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms for data augmentation and refining\n",
    "\n",
    "# train_transforms = transforms.Compose([\n",
    "#     ScaleIntensityd(keys = ['ct_generated', 'std_map']), \n",
    "#     RandRotated(keys = ['ct_generated', 'std_map'], range_x=(-np.pi/12, np.pi/12), prob = 0.5, keep_size = True), \n",
    "#     RandFlipd(keys = ['ct_generated', 'std_map'], spatial_axis = 0, prob = 0.5), \n",
    "#     RandZoomd(keys = ['ct_generated', 'std_map'], zoom = (0.9,1.1), prob = 0.5) \n",
    "# ])\n",
    "\n",
    "# val_transforms = transforms.Compose([\n",
    "#     ScaleIntensityd(keys = ['ct_generated', 'std_map']) \n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms for data augmentation and refining\n",
    "\n",
    "# train_transforms = v2.Compose([v2.RandomHorizontalFlip(p=0.5),\n",
    "#                             v2.RandomVerticalFlip(p=0.5),\n",
    "#                             v2.RandomRotation(30),\n",
    "#                             v2.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),\n",
    "#                             v2.ToDtype(torch.float32, scale=True),\n",
    "#                             #v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#                             v2.Normalize(mean=[0.456], std=[0.224])\n",
    "# ])\n",
    "\n",
    "# # val_transforms = v2.Compose([v2.RandomHorizontalFlip(p=0.5),\n",
    "# #                              v2.RandomVerticalFlip(p=0.5),\n",
    "# #                             v2.RandomRotation(30),\n",
    "# #                             v2.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),\n",
    "# #                             v2.ToDtype(torch.float32, scale=True),\n",
    "# #                             #v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# #                             v2.Normalize(mean=[0.456], std=[0.224])\n",
    "# # ])\n",
    "\n",
    "# val_transforms = v2.Compose([\n",
    "#     v2.Resize((64,64)),  \n",
    "#     v2.ToTensor(), \n",
    "#     v2.Normalize(mean=[0.456], std=[0.224]) \n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b1f1a0",
   "metadata": {},
   "source": [
    "## Section-4: Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def8e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complete your code here\n",
    "\n",
    "train_ds = CacheDataset(data=train_files, transform=None, cache_rate=1.0)\n",
    "\n",
    "\n",
    "\n",
    "# for data in train_loader:\n",
    "#     ct_generated = data['ct_generated']\n",
    "#     std_map = data['std_map']\n",
    "\n",
    "#     print(\"Batch shape of ct_gen:\", ct_generated.shape)\n",
    "#     print(\"Batch shape of std_map:\", std_map.shape)\n",
    "#     break\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=1, collate_fn=pad_list_data_collate)\n",
    "for data in train_loader:\n",
    "    ct_generated = data['ct_generated']\n",
    "    std_map = data['std_map']\n",
    "    # signal = data['signal']\n",
    "    print(ct_generated.shape)\n",
    "    print(std_map.shape)\n",
    "    # print(signal.shape)\n",
    "    break\n",
    "val_ds = CacheDataset(data=val_files, transform=None, cache_rate=1.0)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=1, collate_fn=pad_list_data_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca141c2",
   "metadata": {},
   "source": [
    "## Section-5: UNet Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e00f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unet with Monte Carlo Dropout\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.conv_op = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_op(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels, dropout_prob=dropout_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels // 2 + out_channels, out_channels, dropout_prob=dropout_prob)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.inc = DoubleConv(n_channels, 64, dropout_prob=dropout_prob)\n",
    "        self.down1 = Down(64, 128, dropout_prob=dropout_prob)\n",
    "        self.down2 = Down(128, 256, dropout_prob=dropout_prob)\n",
    "        self.down3 = Down(256, 512, dropout_prob=dropout_prob)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor, dropout_prob=dropout_prob)\n",
    "        self.up1 = Up(1024 // factor, 512 // factor, bilinear, dropout_prob=dropout_prob)\n",
    "        self.up2 = Up(512 // factor, 256 // factor, bilinear, dropout_prob=dropout_prob)\n",
    "        self.up3 = Up(256 // factor, 128 // factor, bilinear, dropout_prob=dropout_prob)\n",
    "        self.up4 = Up(128, 64, bilinear, dropout_prob=dropout_prob)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    def mc_dropout_forward(self, x, n_samples=30):\n",
    "        self.train() \n",
    "        mc_samples = torch.stack([self.forward(x) for _ in range(n_samples)])\n",
    "        mc_mean = mc_samples.mean(dim=0)\n",
    "        mc_std = mc_samples.std(dim=0)\n",
    "        return mc_mean, mc_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unet with  Bayesian network\n",
    "'''\n",
    "Perform multiple forward passes on the input data. After each propagation, model output is collected.\n",
    "Calculate the mean and standard deviation of the output, which represents the central tendency and variation of the predictions.\n",
    "Calculate the loss and KL divergence of each propagation, which measures the information loss of the model parameter distribution. Adding the loss and KL divergence gives ELBO, which I think is a trade-off between the model fitting the data and keeping the parameter distribution simple.\n",
    "Finally, the average loss (as an estimate of ELBO) and average KL divergence over all propagations are returned, along with the mean and standard deviation of the predictions.\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from blitz.modules import BayesianConv2d\n",
    "from blitz.utils import variational_estimator\n",
    "\n",
    "@variational_estimator\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.conv_op = nn.Sequential(\n",
    "            BayesianConv2d(in_channels, mid_channels, kernel_size=(3,3), padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            BayesianConv2d(mid_channels, out_channels, kernel_size=(3,3), padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_op(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels // 2 + out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = BayesianConv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "    \n",
    "    def sample_elbo(self, inputs, labels, criterion, sample_nbr=10):\n",
    "        total_loss = 0\n",
    "        kl_divergence = 0\n",
    "        predictions = []\n",
    "\n",
    "        for _ in range(sample_nbr):\n",
    "            output = self.forward(inputs)\n",
    "            predictions.append(output)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            kl = sum([layer.kl_divergence for layer in self.modules() if hasattr(layer, 'kl_divergence')])\n",
    "            kl_divergence += kl\n",
    "\n",
    "            elbo = loss + kl\n",
    "            total_loss += elbo\n",
    "\n",
    "        mean_preds = torch.stack(predictions).mean(0)\n",
    "        std_preds = torch.stack(predictions).std(0)\n",
    "        return {'pred': mean_preds, 'std': std_preds, 'loss': total_loss / sample_nbr}, kl_divergence / sample_nbr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421666ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the model and optimizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = UNet(n_channels = 1, n_classes = 1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "model_saving_path = './best'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d414f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS Function - Average Relative Error\n",
    "def average_relative_error(output, target):\n",
    "    # Avoid division by zero\n",
    "    nonzero_mask = target != 0\n",
    "    return torch.mean(torch.abs((output[nonzero_mask] - target[nonzero_mask]) / target[nonzero_mask]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Relative Error Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# test_output_2d = torch.rand(64, 64)\n",
    "# test_target_2d = torch.rand(64, 64)\n",
    "# error_2d_single = average_relative_error(test_output_2d, test_target_2d)\n",
    "# print(type(error_2d_single))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675782db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_comparison(ori, std, pred, uncertainty, epoch, save_path):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    axs[0].imshow(ori, cmap='gray')\n",
    "    axs[0].set_title('Original Image')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(std, cmap='hot')\n",
    "    axs[1].set_title('STD Image')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(pred, cmap='hot')\n",
    "    axs[2].set_title('Predicted STD')\n",
    "    axs[2].axis('off')\n",
    "    \n",
    "    axs[3].imshow(uncertainty, cmap='hot')\n",
    "    axs[3].set_title('Uncertainty')\n",
    "    axs[3].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    if save_path:\n",
    "        formatted_path = save_path.format(epoch=epoch)  \n",
    "        fig.savefig(formatted_path, bbox_inches='tight', dpi=150)\n",
    "        plt.close(fig)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed57c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MonteCarlo training and validation\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "num_epochs = 50\n",
    "best_loss = 1.0\n",
    "import cv2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_val_loss = 0.0\n",
    "    model.train()  # Ensure the model is in training mode\n",
    " \n",
    "    for idx, images in enumerate(tqdm(train_loader)):\n",
    "        img = images['ct_generated'].float().to(device)\n",
    "        std_map = images['std_map'].float().to(device)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(img)\n",
    "        \n",
    "        loss = average_relative_error(y_pred, std_map)\n",
    "        if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "        #loss = criterion(y_pred, std_map)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * img.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for idx, image in enumerate(tqdm(val_loader)):\n",
    "            img = image['ct_generated'].float().to(device)\n",
    "            std_map = image['std_map'].float().to(device)\n",
    "            mc_mean, mc_std = model.mc_dropout_forward(img, n_samples=50)\n",
    "            # mc_outputs = model.forward_with_mc_dropout(img, n_passes=30)\n",
    "            # mc_mean = mc_outputs.mean(0)\n",
    "            # mc_std = mc_outputs.std(0)\n",
    "            \n",
    "            # y_pred = model(img)\n",
    "\n",
    "            val_loss = average_relative_error(mc_mean, std_map)\n",
    "\n",
    "            running_val_loss += val_loss.item() * img.size(0)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                pred_mean_np = mc_mean.squeeze().cpu().numpy()\n",
    "                pred_std_np = mc_std.squeeze().cpu().numpy()\n",
    "                img_np = img.squeeze().cpu().numpy()\n",
    "                std_np = std_map.squeeze().cpu().numpy()\n",
    "                plot_comparison(img_np, std_np, pred_mean_np, pred_std_np, epoch=epoch + 1, save_path=\"nunet_results/epoch_{epoch}.png\")\n",
    "\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}\")\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayes training and validation\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "num_epochs = 50\n",
    "best_loss = 1.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_val_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for idx, images in enumerate(tqdm(train_loader)):\n",
    "        img = images['ct_generated'].float().to(device)\n",
    "        std_map = images['std_map'].float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(img)\n",
    "        loss = average_relative_error(y_pred, std_map)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * img.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, image in enumerate(tqdm(val_loader)):\n",
    "            img = image['ct_generated'].float().to(device)\n",
    "            std_map = image['std_map'].float().to(device)\n",
    "\n",
    "            outputs, kl_divergence = model.sample_elbo(inputs=img, labels=std_map, criterion=average_relative_error, sample_nbr=10)\n",
    "            val_loss = outputs['loss']  \n",
    "\n",
    "            running_val_loss += val_loss.item() * img.size(0)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                pred_mean_np = outputs['pred'].mean(0).squeeze().cpu().numpy()\n",
    "                pred_std_np = outputs['pred'].std(0).squeeze().cpu().numpy()\n",
    "                img_np = img.squeeze().cpu().numpy()\n",
    "                std_np = std_map.squeeze().cpu().numpy()\n",
    "                plot_comparison(img_np, std_np, pred_mean_np, pred_std_np, epoch=epoch + 1, save_path=\"nunet_results/epoch_{epoch}.png\")\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "plot_losses(train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6614865",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "noise_nrrd, _ = nrrd.read(os.path.join(data_dir, f\"L506_noise.nrrd\"))\n",
    "\n",
    "noise = np.max(noise_nrrd, axis=1)\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(), \n",
    "                            transforms.Resize((512,512))])\n",
    "\n",
    "noise = trans(noise)\n",
    "k = random.uniform(0,5)\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = UNet(n_channels=1, n_classes=1).to(device)\n",
    "model_path = 'best_bayes.pth'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "volume_2d = volume_2d + (k*noise)\n",
    "volume_2d = volume_2d.squeeze().float()\n",
    "volume_2d = volume_2d.unsqueeze(0).unsqueeze(0)\n",
    "volume_2d = volume_2d.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(volume_2d)\n",
    "\n",
    "output_cpu = output.cpu().numpy().squeeze()  \n",
    "\n",
    "stdvolume, stdheader = nrrd.read(os.path.join(data_dir,'L506_std.nrrd'))\n",
    "stdvolume_2d = convert_to_2d(stdvolume)  \n",
    "stdvolume_2d = trans(stdvolume_2d) \n",
    "\n",
    "if isinstance(stdvolume_2d, torch.Tensor):\n",
    "    stdvolume_2d = stdvolume_2d.cpu().numpy()\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axs[0].imshow(volume_2d.cpu().squeeze().numpy(), cmap='gray')\n",
    "axs[0].set_title('Original Image')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(stdvolume_2d.squeeze(), cmap='hot')\n",
    "axs[1].set_title('STD Image')\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(output_cpu, cmap='hot')\n",
    "axs[2].set_title('Predicted STD')\n",
    "axs[2].axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1330a",
   "metadata": {},
   "source": [
    "## Section-6: RATUNet Training and Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
